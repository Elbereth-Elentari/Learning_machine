"verb, present tense with 3rd person singular",VBZ,5
"Return a list of tuples, whose 1st element is a word, while 2nd - its POS tag. The input is a list of words.",pos_tag(),5
"Import a specified grammar (called ""grammar"") into regex parser.",nltk.RegexpParser(grammar),5
"Print synsets of the word ""dog"".","wordnet.synsets(""dog"")",5
Verify that the django project in path works.,python manage.py runserver,5
Create a polls app.,python manage.py startapp polls,5
Import HTTP Response.,from django.http import HttpResponse,5
a string that contains a URL pattern,route,5
"Naming your URL lets you refer to it unambiguously from elsewhere in Django, especially from within templates. This powerful feature allows you to make global changes to the URL patterns of your project while only touching a single file.",name,5
An authentication system.,django.contrib.auth,5
A framework for content types.,django.contrib.contenttypes,5
A messaging framework.,django.contrib.messages,5
"Import ""Page not Found"".",from django.http import Http404,5
"Import ""get object or 404"".",from django.shortcuts import get_object_or_404,5
Import HTTP Response Redirect.,from django.http import HttpResponseRedirect,5
Import reverse.,from django.urls import reverse,5
This function helps avoid having to hardcode a URL in the view function. It is given the name of the view that we want to pass control to and the variable portion of the URL pattern that points to that view.,reverse(),5
This view displays a list of polls.,index(),5
Import generic.,from django.views import generic,5
"A token that performs a control function. It is a newline or one of the following: ‘||’, ‘&&’, ‘&’, ‘;’, ‘;;’, ‘;&’, ‘;;&’, ‘|’, ‘|&’, ‘(’, or ‘)’.",control operator,5
"A character that, when unquoted, separates words. A metacharacter is a space, tab, newline, or one of the following characters: ‘|’, ‘&’, ‘;’, ‘(’, ‘)’, ‘<’, or ‘>’.",metacharacter,5
"A control operator or a redirection operator. See Redirections, for a list of redirection operators. Operators contain at least one unquoted metacharacter.",operator,5
carriage return,\r,5
A Dense layer with 128 neurons and relu activation.,"tf.keras.layers.Dense(128, activation='relu')",5
a high-level API to build and train models in TensorFlow,tf.keras,5
Used to monitor the training and testing steps.,metrics,5
Save the model as a SavedModel.,model.save('saved_model/my_model'),5
URL path to a sample train dataset in CSV.,"TRAIN_DATA_URL = ""https://storage.googleapis.com/tf-datasets/titanic/train.csv""",5
Make numpy values easier to read.,"np.set_printoptions(precision=3, suppress=True)",5
Import the Keys class.,from selenium.webdriver.common.keys import Keys,5
Waiting condition that the title contains a certain string.,title_contains,5
Select three initial rows in df.,df[0:3],5
"Write df to foo.xlsx, sheet Sheet1.","df.to_excel(""foo.xlsx"", sheet_name=""Sheet1"")",5
"adjective, superlative",JJS,5
"proper noun, singular",NNP,5
"predeterminer (all, both, half)",PDT,5
verb past tense,VBD,5
verb past participle,VBN,5
import stemmer,from nltk.stem import PorterStemmer,5
"Parse ""tag"" (POS-tagged tokens) with rule cp.",cp.parse(tag),5
Draw a syntactic tree of result.,result.draw(),5
Return the frequency distribution of a list of words.,nltk.FreqDist(words),5
Return a list of all bigrams made up of Tokens.,list(nltk.bigrams(Tokens)),5
Return a list of all trigrams made up of Tokens.,list(nltk.trigrams(Tokens)),5
Import path for django urls.,from django.urls import path,5
A command-line utility that lets you interact with this Django project in various ways.,manage.py,5
An entry-point for ASGI-compatible web servers to serve your project.,mysite/asgi.py,5
"This function allows referencing other URLconfs. Whenever Django encounters the function, it chops off whatever part of the URL matched up to that point and sends the remaining string to the included URLconf for further processing.
The idea behind this function is to make it easy to plug-and-play URLs. Since polls are in their own URLconf (polls/urls.py), they can be placed under “/polls/”, or under “/fun_polls/”, or under “/content/polls/”, or any other path root, and the app will still work.",include(),5
Arbitrary keyword arguments can be passed in a dictionary to the target view.,kwargs,5
A session framework.,django.contrib.sessions,5
Import Question.,from .models import Question,5
tell the admin that Question objects have an admin interface,admin.site.register(Question),5
all POST forms that are targeted at internal URLs should use this template tag,{% csrf_token %},5
"After somebody votes in a question, this view redirects to the results page for the question.",vote(),5
A space or tab character.,blank,5
"The value returned by a command to its caller. The value is restricted to eight bits, so the maximum value is 255.",exit status,5
A string of characters used to identify a file.,filename,5
A synonym for exit status.,return status,5
alert (bell),\a,5
question mark,\?,5
"First, the arithmetic expression expr1 is evaluated according to the rules described below (see Shell Arithmetic). The arithmetic expression expr2 is then evaluated repeatedly until it evaluates to zero. Each time expr2 evaluates to a non-zero value, commands are executed and the arithmetic expression expr3 is evaluated. If any expression is omitted, it behaves as if it evaluates to 1. The return value is the exit status of the last command in commands that is executed, or false if any of the expressions is invalid.",for (( expr1 ; expr2 ; expr3 )) ; do commands ; done,5
Evaluate model with verbosity level 2.,"model.evaluate(x_test,  y_test, verbose=2)",5
Import Keras.,from tensorflow import keras,5
collection of examples for simultaneous evaluation,batch,5
Locate all elements with a specific tag name.,driver.find_elements_by_tag_name(),5
Sort by values in column B.,"df.sort_values(by=""B"")",5
Write df to file foo.csv.,"df.to_csv(""foo.csv"")",5
particle (about),RP,5
Create urlpatterns from the index of views.,"urlpatterns = [path('', views.index, name='index'),]",5
"This function is passed four arguments, two required: route and view, and two optional: kwargs, and name.",path(),5
a dictionary-like object that lets you access submitted data by key name,request.POST,5
"adverb, superlative",RBS,5
"Create HTTP Response ""Hello, world.""","HttpResponse(""Hello, world."")",5
The URL declarations for this Django project; a “table of contents” of your Django-powered site,mysite/urls.py,5
create the tables in the database,python manage.py migrate,5
indicates how many times the for tag has gone through its loop,forloop.counter,5
"A unit of text that is the result of one of the shell expansions. After expansion, when executing a command, the resulting fields are used as the command name and arguments.",field,5
A mechanism by which users can selectively stop (suspend) and restart (resume) execution of processes.,job control,5
A collection of related processes each having the same process group ID.,process group,5
"A word that has a special meaning to the shell. Most reserved words introduce shell flow control constructs, such as for and while.",reserved word,5
A sequence of characters considered a single unit by the shell. It is either a word or an operator.,token,5
"Bash escape character. It preserves the literal value of the next character that follows, with the exception of newline. If a \newline pair appears, and the backslash itself is not quoted, the \newline is treated as a line continuation (that is, it is removed from the input stream and effectively ignored).",\,5
" preserves the literal value of all characters within the quotes, with the exception of ‘$’, ‘`’, ‘\’, and, when history expansion is enabled, ‘!’. When the shell is in POSIX mode (see Bash POSIX Mode), the ‘!’ has no special meaning within double quotes, even when history expansion is enabled. The characters ‘$’ and ‘`’ retain their special meaning within double quotes (see Shell Expansions). The backslash retains its special meaning only when followed by one of the following characters: ‘$’, ‘`’, ‘""’, ‘\’, or newline. Within double quotes, backslashes that are followed by one of these characters are removed. Backslashes preceding characters without a special meaning are left unmodified. A double quote may be quoted within double quotes by preceding it with a backslash. If enabled, history expansion will be performed unless an ‘!’ appearing in double quotes is escaped using a backslash. The backslash preceding the ‘!’ is not removed.
The special parameters ‘*’ and ‘@’ have special meaning when in double quotes (see Shell Parameter Expansion).","""",5
an escape character (not ANSI C),\E,5
newline,\n,5
vertical tab,\v,5
single quote,\',5
a sequence of one or more commands separated by one of the control operators ‘|’ or ‘|&’.,pipeline,5
"Execute consequent-commands as long as test-commands has an exit status which is not zero. The return status is the exit status of the last command executed in consequent-commands, or zero if none was executed.",until test-commands; do consequent-commands; done,5
Import TensorFlow.,import tensorflow as tf,5
Load the MNIST dataset.,"(x_train, y_train), (x_test, y_test) = mnist.load_data()",5
A Dropout layer of 0.2.,tf.keras.layers.Dropout(0.2),5
"Compile model with Adam optimiser, sparce categorical entropy as the loss function and accuracy as metric.","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])",5
Print the version of TensorFlow.,print(tf.__version__),5
"This measures how accurate the model is during training. You want to minimize this function to ""steer"" the model in the right direction.",loss function,5
Use model to predict the labels for test_images.,predictions = model.predict(test_images),5
Create a callback that saves the model's weights,"cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=1)",5
Install dependencies for saving models in HDF5 format.,pip install -q pyyaml h5py,5
load the weights from the checkpoint ,model.load_weights(checkpoint_path),5
Name the directory after the checkpoint_path.,checkpoint_dir = os.path.dirname(checkpoint_path),5
Load the latest weights to model.,model.load_weights(latest),5
Manually restore weights.,model.load_weights('./checkpoints/my_checkpoint'),5
URL path to a sample test dataset in CSV.,"TEST_DATA_URL = ""https://storage.googleapis.com/tf-datasets/titanic/eval.csv""",5
"Set the list of labels as [0, 1].","LABELS = [0, 1]",5
Basic normalization.,(data-mean)/std,5
"Assuming you have an array of labels, pass it as a tuple into tf.data.Dataset.from_tensor_slices to create a tf.data.Dataset.","test_dataset = tf.data.Dataset.from_tensor_slices((test_examples, test_labels))",5
Set the shuffle buffer size to 100.,SHUFFLE_BUFFER_SIZE = 100,5
Start the mysite project in django.,django-admin startproject mysite,5
Settings/configuration for this Django project.,mysite/settings.py,5
"When Django finds a matching pattern, it calls the specified view function with an HttpRequest object as the first argument and any “captured” values from the route as keyword arguments.",view,5
the act of submitting this form will alter data server-side,"method=""post""",5
Import Choice.,from .models import Choice,5
"A set of processes comprising a pipeline, and any processes descended from it, that are all in the same process group.",job,5
"A word consisting solely of letters, numbers, and underscores, and beginning with a letter or underscore. Names are used as shell variable and function names. Also referred to as an identifier.",name,5
A unique identifier that represents a process group during its lifetime.,process group ID,5
A shell builtin command that has been classified as special by the POSIX standard.,special builtin,5
preserves the literal value of each character within it,',5
backspace,\b,5
form feed,\f,5
horizontal tab,\t,5
backslash,\\,5
the Unicode (ISO/IEC 10646) character whose value is the hexadecimal value HHHH (one to four hex digits),\uHHHH,5
" a sequence of one or more pipelines separated by one of the operators ‘;’, ‘&’, ‘&&’, or ‘||’, and optionally terminated by one of ‘;’, ‘&’, or a newline.",list,5
Assign the MNIST dataset to the variable mnist.,mnist = tf.keras.datasets.mnist,5
A Dense layer with 10 neurons and softmax activation.,"tf.keras.layers.Dense(10, activation='softmax')",5
Import PyPlot.,import matplotlib.pyplot as plt,5
Assign the Fashion MNIST dataset to variable fashion_mnist.,fashion_mnist = keras.datasets.fashion_mnist,5
Returns the highest confidence value for the first prediction array.,np.argmax(predictions[0]),5
Display the model's architecture,model.summary(),5
Manually save weights.,model.save_weights('./checkpoints/my_checkpoint'),5
Download sample CSV train dataset.,"train_file_path = tf.keras.utils.get_file(""train.csv"", TRAIN_DATA_URL)",5
Download sample CSV test dataset.,"test_file_path = tf.keras.utils.get_file(""eval.csv"", TEST_DATA_URL)",5
Display a batch from the dataset.,show_batch(dataset),5
Set the batch size to 64.,BATCH_SIZE = 64,5
Shuffle and batch the train set.,train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE),5
Batch the test set.,test_dataset = test_dataset.batch(BATCH_SIZE),5
Print the data types in df.,df.dtypes,5
Import TensorFlow Datasets.,import tensorflow_datasets as tfds,5
Create a smaller test set.,test_data = all_encoded_data.take(TAKE_SIZE),5
"The next layer is a Long Short-Term Memory layer, which lets the model understand words in their context with other words. A bidirectional wrapper on the LSTM helps it to learn about the datapoints in relationship to the datapoints that came before it and after it.",model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))),5
The basic TensorFlow dtype allows you to build tensors of byte strings. Unicode strings are utf-8 encoded by default.,tf.string,5
A sequence of characters treated as a unit by the shell. Words may not include unquoted metacharacters.,word,5
Build the Sequential model with a list of layers.,model = tf.keras.models.Sequential([]),5
This is how the model is updated based on the data it sees and its loss function.,optimizer,5
Train the model with the cp_callback,"model.fit(train_images, train_labels, epochs=10, validation_data=(test_images,test_labels), callbacks=[cp_callback])",5
creates a single collection of TensorFlow checkpoint files that are updated at the end of each epoch,ls {checkpoint_dir},5
Include the epoch in the file name (uses `str.format`),"checkpoint_path = ""training_2/cp-{epoch:04d}.ckpt""",5
Save the weights using the `checkpoint_path` format,model.save_weights(checkpoint_path.format(epoch=0)),5
Save the model to a H5 file.,model.save('my_model.h5'),5
Recreate new_model from file my_model.h5.,new_model = tf.keras.models.load_model('my_model.h5'),5
"Assuming you have an array of examples, pass it as a tuple into tf.data.Dataset.from_tensor_slices to create a tf.data.Dataset.","train_dataset = tf.data.Dataset.from_tensor_slices((train_examples, train_labels))",5
Create an encoder by passing the vocabulary_set to TokenTextEncoder. The encoder's encode method takes in a string of text and returns a list of integers.,encoder = tfds.features.text.TokenTextEncoder(vocabulary_set),5
create a larger training set,train_data = all_encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE),5
"Before being passed into the model, the datasets need to be batched. Typically, the examples inside of a batch need to be the same size and shape. But, the examples in these datasets are not all the same size — each line of text had a different number of words.","train_data = train_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]))",5
The first layer converts integer representations to dense vector embeddings.,"model.add(tf.keras.layers.Embedding(vocab_size, 64))",5
Is the string length is included in the tensor dimensions of a tf.string?,no,5
here the sequence of code points is encoded using a known character encoding,string scalar,5
Converts an encoded string scalar to a vector of code points.,tf.strings.unicode_decode,5
Converts a vector of code points to an encoded string scalar.,tf.strings.unicode_encode,5
Converts an encoded string scalar to a different encoding.,tf.strings.unicode_transcode,5
Convert batch_chars_ragged tensor to a dense tf.Tensor with padding.,batch_chars_padded = batch_chars_ragged.to_tensor(default_value=-1),5
Display the batch_chars_padded dense tensor.,print(batch_chars_padded.numpy()),5
Convert batch_chars_ragged tensor to a sparse tf.Tensor.,batch_chars_sparse = batch_chars_ragged.to_sparse(),5
"When encoding multiple strings with varyling length, a tf.RaggedTensor should be used as input","tf.strings.unicode_encode(batch_chars_ragged, output_encoding='UTF-8')",5
"If you have a tensor with multiple strings in sparse format, then convert it to a tf.RaggedTensor before calling unicode_encode","tf.strings.unicode_encode(tf.RaggedTensor.from_sparse(batch_chars_sparse), output_encoding='UTF-8')",5
"If you have a tensor with multiple strings in padded format, then convert it to a tf.RaggedTensor before calling unicode_encode","tf.strings.unicode_encode(tf.RaggedTensor.from_tensor(batch_chars_padded, padding=-1), output_encoding='UTF-8')",5
Import TensorFlow Text.,import tensorflow_text as text,5
Set the whitespace tokenizer to the tokenizer value.,tokenizer = text.WhitespaceTokenizer(),5
Tokenize a list of strings with tokenizer.,tokens = tokenizer.tokenize([]),5
Print tokens as a list.,print(tokens.to_list()),5
"Return example, label pair.","return example, tf.cast(index, tf.int64)",5
"When tokenizing languages without whitespace to segment words, it is common to just split by character.","tokens = tf.strings.unicode_split([u""string"".encode('UTF-8')], 'UTF-8')",5
"Returns True/False values for each token in tokens, for whether they're capitalised or not.","text.wordshape(tokens, text.WordShape.HAS_TITLE_CASE)",5
"Returns True/False values for each token in tokens, for whether they're all uppercase or not.","text.wordshape(tokens, text.WordShape.IS_UPPERCASE)",5
"Returns True/False values for each token in tokens, for whether they contain some punctuation or symbol.","text.wordshape(tokens, text.WordShape.HAS_SOME_PUNCT_OR_SYMBOL)",5
"Returns True/False values for each token in tokens, for whether they are numbers.","text.wordshape(tokens, text.WordShape.IS_NUMERIC_VALUE)",5
"Divide tokens by space, into bigrams.","bigrams = text.ngrams(tokens, 2, reduction_type=text.Reduction.STRING_JOIN)",5
Convert the MNIST samples from integers to floating-point numbers,"x_train, x_test = x_train / 255.0, x_test / 255.0",5
Train model with 5 epochs.,"model.fit(x_train, y_train, epochs=5)",5
Choose the latest checkpoint.,latest = tf.train.latest_checkpoint(checkpoint_dir),5
here each position contains a single code point,int32 vector,5
"When tokenizing strings, it is often desired to know where in the original string the token originated from. For this reason, each tokenizer which implements TokenizerWithOffsets has a method that will return the byte offsets along with the tokens.","(tokens, offset_starts, offset_limits) = tokenizer.tokenize_with_offsets([])",5
a simple format for storing a sequence of binary records,TFRecord,5
"a cross-platform, cross-language library for efficient serialization of structured data",Protocol buffers,5
"defined by .proto files, these are often the easiest way to understand a message type",Protocol messages,5
"a flexible message type that represents a {""string"": value} mapping. It is designed for use with TensorFlow and is used throughout the higher-level APIs such as TFX.",tf.Example,5
"A Python dictionary in which:
Each key is the name of a feature.
Each value is an array containing all of that feature's values.",features,5
An array containing the values of the label for every example.,label,5
Convert the inputs to a Dataset.,"dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))",5
A classifier Estimator for deep models that perform multi-class classification.,tf.estimator.DNNClassifier,5
Import ROC curve.,from sklearn.metrics import roc_curve,5
Is a Tensor mutable?,no,5
A vector with mostly 0 values.,sparse,5
Create a callback that saves the model's weights every 5 epochs,"cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, Verbose=1, save_weights_only=True, period=5)",5
Output layer. The first argument is the number of labels.,"model.add(tf.keras.layers.Dense(3, activation='softmax'))",5
Shuffle dataset and repeat.,dataset = dataset.shuffle(1000).repeat(),5
A classifier Estimator for wide & deep models.,tf.estimator.DNNLinearCombinedClassifier,5
A classifier Estimator for classifiers based on linear models.,tf.estimator.LinearClassifier,5
Return the lowest index in string h where substring i is found within the slice s[start:end]. Optional arguments start and end are interpreted as in slice notation. Return -1 if substring is not found.,"h.find(i, start, end)",5
Remove element elem from the set. Raises KeyError if elem is not contained in the set.,remove(elem),5
"a dense vector of floating point values (the length of the vector is a parameter you specify). Instead of specifying the values for the embedding manually, they are trainable parameters (weights learned by the model during training, in the same way a model learns weights for a dense layer). It is common to see word embeddings that are 8-dimensional (for small datasets), up to 1024-dimensions when working with large datasets. A higher dimensional embedding can capture fine-grained relationships between words, but takes more data to learn.",embedding,5
Import layers.,from tensorflow.keras import layers,5
Represents spaces in the subword vocabulary.,_,5
The lowercase letters 'abcdefghijklmnopqrstuvwxyz'.,string.ascii_lowercase,5
"Compile model with Adam optimiser, binary crossentropy as loss and accuracy as metric.","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])",5
"Train model with train_batches, 10 epochs, test_batches and 20 validation steps. Save into variable history.","history = model.fit(train_batches, epochs=10, validation_data=test_batches, validation_steps=20)",5
Save the dictionary of history.,history_dict = history.history,5
Save the values of 'accuracy' into the 'acc' variable.,acc = history_dict['accuracy'],5
Save the values of 'val_accuracy' into the 'val_acc' variable.,val_acc = history_dict['val_accuracy'],5
Save the values of 'loss' into the 'loss' variable.,loss = history_dict['loss'],5
Save the values of 'val_loss' into the 'val_loss' variable.,val_loss = history_dict['val_loss'],5
"Name the figure ""Training and validation accuracy"".",plt.title('Training and validation accuracy'),5
"Label the y axis ""Accuracy"".",plt.ylabel('Accuracy'),5
"Create a dataset from file_path, with batch size 5, NA's as ?, with a single epoch.","dataset = tf.data.experimental.make_csv_dataset(file_path, batch_size=5, label_name=LABEL_COLUMN, na_value=""?"", num_epochs=1, ignore_errors=True, **kwargs)",5
"This tokenizer splits UTF-8 strings based on Unicode script boundaries. The script codes used correspond to International Components for Unicode (ICU) UScriptCode values. See: http://icu-project.org/apiref/icu4c/uscript_8h.html

In practice, this is similar to the WhitespaceTokenizer with the most apparent difference being that it will split punctuation (USCRIPT_COMMON) from language texts (eg. USCRIPT_LATIN, USCRIPT_CYRILLIC, etc) while also separating language texts from each other.",tokenizer = text.UnicodeScriptTokenizer(),5
an object describing how the model should use raw input data from the features dictionary,feature column,5
"The Embedding layer can be understood as a lookup table that maps from integer indices (which stand for specific words) to dense vectors (their embeddings). The dimensionality (or width) of the embedding is a parameter you can experiment with to see what works well for your problem, much in the same way you would experiment with the number of neurons in a Dense layer.   For text or sequence problems, the Embedding layer takes a 2D tensor of integers, of shape (samples, sequence_length), where each entry is a sequence of integers. It can embed sequences of variable lengths. You could feed into the embedding layer above batches with shapes (32, 10) (batch of 32 sequences of length 10) or (64, 15) (batch of 64 sequences of length 15).","embedding_layer = layers.Embedding(1000, 5)",5
"a GlobalAveragePooling1D layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input of variable length, in the simplest way possible.",layers.GlobalAveragePooling1D(),5
"The last layer is densely connected with a single output node. Using the sigmoid activation function, this value is a float between 0 and 1, representing a probability (or confidence level) that the review is positive.","layers.Dense(1, activation='sigmoid')",5
"Label the x axis ""Epochs"".",plt.xlabel('Epochs'),5
Place the figure legend in the lower right-hand corner.,plt.legend(loc='lower right'),5
"Limit the y axis to (0.5,1).","plt.ylim((0.5,1))",5
This fixed-length output vector is piped through a fully-connected (Dense) layer with 16 hidden units.,"layers.Dense(16, activation='relu')",5
Set the figure size to 1200 x 900 pixels.,"plt.figure(figsize=(12,9))",5
Plot accuracy by epochs.,"plt.plot(epochs, acc, 'bo', label='Training acc')",5
Plot validation accuracy by epochs.,"plt.plot(epochs, val_acc, 'b', label='Validation acc')",5
Display the figure.,plt.show(),5
Assign the first layer to variable e.,e = model.layers[0],5
Retrieve weights learned at layer e.,weights = e.get_weights()[0],5
"Print the word embeddings learned during training. This will be a matrix of shape (vocab_size, embedding-dimension).",print(weights.shape),5
Print the vocabulary size in encoder.,print ('Vocabulary size: {}'.format(encoder.vocab_size)),5
Import Time.,import time,5
"Assign a set of index, vocab element to variable char2idx.","char2idx = {u:i for i, u in enumerate(vocab)}",5
Set the maximum length sentence we want for a single input in characters to 100.,seq_length = 100,5
Assign the text features to variable encoder.,encoder = info.features['text'].encoder,5
Download the Shakespeare dataset.,"path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')",5
"Create a lookup table mapping characters to numbers, save it as text_as_int.",text_as_int = np.array([char2idx[c] for c in text]),5
Create training examples / targets.,char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int),5
The batch method lets us easily convert these individual characters to sequences of the desired size.,"sequences = char_dataset.batch(seq_length+1, drop_remainder=True)",5
Set the embedding dimension to 256.,embedding_dim = 256,5
Set the number of RNN units to 1024.,rnn_units = 1024,5
"The output layer, with vocab_size outputs.",tf.keras.layers.Dense,5
The input layer. A trainable lookup table that will map the numbers of each character to a vector with embedding_dim dimensions,tf.keras.layers.Embedding,5
"Attach an optimizer, and a loss function
The standard tf.keras.losses.sparse_categorical_crossentropy loss function works in this case because it is applied across the last dimension of the predictions.

Because our model returns logits, we need to set the from_logits flag.","example_batch_loss = tf.keras.losses.sparse_categorical_crossentropy(target_example_batch, example_batch_predictions, from_logits=True)",5
Empty string to store our results,text_generated = [],5
Low temperatures results in more predictable text. Higher temperatures results in more surprising text. Experiment to find the best setting.,temperature = 1.0,5
Reset the model's states.,model.reset_states(),5
Append the next character to text_generated.,text_generated.append(idx2char[predicted_id]),5
Return the start_string followed by the generated text.,return (start_string + ''.join(text_generated)),5
Build a DNN with 2 hidden layers with 30 and 10 hidden nodes each. The model must choose between 3 classes.,"classifier = tf.estimator.DNNClassifier(feature_columns=my_feature_columns, hidden_units=[30, 10], n_classes=3)",5
Train the classifier model.,"classifier.train(input_fn=lambda: input_fn(train, train_y, training=True), steps=5000)",5
Load IMDB dataset.,"(train_data, test_data), info = tfds.load('imdb_reviews/subwords8k', split = (tfds.Split.TRAIN, tfds.Split.TEST), with_info=True, as_supervised=True)",5
Subword encoder.,tfds.features.text.SubwordTextEncoder,5
"the Embedding layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. The resulting dimensions are: (batch, sequence, embedding).","layers.Embedding(encoder.vocab_size, embedding_dim)",5
A type of RNN with size units=rnn_units (You can also use a LSTM layer here.),tf.keras.layers.GRU,5
"An Embedding layer with batch input shape [batch_size, None]","tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None])",5
A GRU layer with recurrent initialiser glorot uniform.,"tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform')",5
Set the number of characters to generate to 1000.,num_generate = 1000,5
remove the batch dimension,"predictions = tf.squeeze(predictions, 0)",5
using a categorical distribution to predict the word returned by the model,"predicted_id = tf.random.categorical(predictions / temperature, num_samples=1)[-1,0].numpy()",5
"We pass the predicted word as the next input to the model, along with the previous hidden state","input_eval = tf.expand_dims([predicted_id], 0)",5
"Print the output of generate_text from model, with u""ROMEO: "" as the starting string.","print(generate_text(model, start_string=u""ROMEO: ""))",5
A container for accessing linguistic annotations.,Doc,5
A slice from a Doc object.,Span,5
"An individual token — i.e. a word, punctuation symbol, whitespace, etc.",Token,5
"An entry in the vocabulary. It’s a word type with no context, as opposed to a word token. It therefore has no part-of-speech tag, dependency parse etc.",Lexeme,5
A text-processing pipeline. Usually you’ll load this once per process as nlp and pass the instance around your application.,Language,5
"Segment text, and create Doc objects with the discovered segment boundaries.",Tokenizer,5
Determine the base forms of words.,Lemmatizer,5
"Assign linguistic features like lemmas, noun case, verb tense etc. based on the word and its part-of-speech tag.",Morphology,5
Annotate syntactic dependencies on Doc objects.,DependencyParser,5
Annotate part-of-speech tags on Doc objects.,Tagger,5
"Annotate named entities, e.g. persons or products, on Doc objects.",EntityRecognizer,5
"Match sequences of tokens, based on pattern rules, similar to regular expressions.",Matcher,5
Match sequences of tokens based on phrases.,PhraseMatcher,5
Add entity spans to the Doc using token-based rules or exact phrase matches.,EntityRuler,5
Implement custom sentence boundary detection logic that doesn’t require the dependency parse.,Sentencizer,5
A lookup table for the vocabulary that allows you to access Lexeme objects.,Vocab,5
Map strings to and from hash values.,StringStore,5
Container class for vector data keyed by string.,Vectors,5
"An annotated corpus, using the JSON file format. Manages annotations for tagging, dependency parsing and NER.",GoldCorpus,5
Import English.,from spacy.lang.en import English,5
Set language to English.,nlp = English(),5
Assign the tokens variable to tokenized string.,tokens = nlp(),5
List of tokens.,tokens_text = [t.text for t in tokens],5
adverbial clause modifier,advcl,5
adverbial modifier,advmod,5
adjectival modifier,amod,5
appositional modifier,appos,5
auxiliary,aux,5
Converting our start string to numbers (vectorizing),"input_eval = tf.expand_dims([char2idx[s] for s in start_string], 0)",5
Assign categories or labels to Doc objects.,TextCategorizer,5
Collection for training annotations.,GoldParse,5
clausal modifier of noun (adjectival clause),acl,5
case marking,case,5
coordinating conjunction,cc,5
clausal complement,ccomp,5
compound,compound,5
conjunct,conj,5
copula,cop,5
clausal subject (passive),csubjpass,5
determiner,det,5
direct object,dobj,5
expletive,expl,5
marker,mark,5
meta modifier,meta,5
negation modifier,neg,5
noun compound modifier,nn,5
modifier of nominal,nounmod,5
classifier,clf,5
clausal subject,csubj,5
dative,dative,5
unclassified dependent,dep,5
interjection,intj,5
noun phrase as adverbial modifier,npmod,5
nominal subject,nsubj,5
nominal subject (passive),nsubjpass,5
numeric modifier,nummod,5
object,obj,5
oblique nominal,obl,5
parataxis,parataxis,5
object predicate,oprd,5
complement of preposition,pcomp,5
object of preposition,pobj,5
possession modifier,poss,5
prepositional modifier,prep,5
particle,prt,5
punctuation,punct,5
modifier of quantifier,quantmod,5
relative clause modifier,relcl,5
root,root,5
open clausal complement,xcomp,5
"People, including fictional.",PERSON,5
Nationalities or religious or political groups.,NORP,5
"Buildings, airports, highways, bridges, etc.",FAC,5
"Companies, agencies, institutions, etc.",ORG,5
"Countries, cities, states.",GPE,5
"Non-GPE locations, mountain ranges, bodies of water.",LOC,5
"Objects, vehicles, foods, etc. (Not services.)",PRODUCT,5
"Named hurricanes, battles, wars, sports events, etc.",EVENT,5
"Titles of books, songs, etc.",WORK_OF_ART,5
Named documents made into laws.,LAW,5
Any named language.,LANGUAGE,5
Absolute or relative dates or periods.,DATE,5
Times smaller than a day.,TIME,5
"Percentage, including ”%“.",PERCENT,5
"Monetary values, including unit.",MONEY,5
"Measurements, as of weight or distance.",QUANTITY,5
"“first”, “second”, etc.",ORDINAL,5
Numerals that do not fall under another type.,CARDINAL,5
"To convert one or more existing Doc objects to spaCy’s JSON format, you can use this helper.",gold.docs_to_json,5
"To populate a model’s vocabulary, you can use this command and load in a newline-delimited JSON (JSONL) file containing one lexical entry per line via the --jsonl-loc option. The first line defines the language and vocabulary settings. All other lines are expected to be JSON objects describing an individual lexeme. The lexical attributes will be then set as attributes on spaCy’s Lexeme object. ",spacy init model,5
Construct a Doc object. The most common way to get a Doc object is via the nlp object.,Doc.__init__,5
A storage container for lexical types.,vocab,5
A list of strings to add to the container.,words,5
"A list of boolean values indicating whether each word has a subsequent space. Must have the same length as words, if specified. Defaults to a sequence of True.",spaces,5
"Get a Doc object from ""Some text"". Save as variable doc.","doc = nlp(""Some text"")",5
Import Doc.,from spacy.tokens import Doc,5
pre-correlative conjunction,preconj,5
spaCy takes training data in JSON format. This built-in command helps you convert the .conllu format used by the Universal Dependencies corpora to spaCy’s training format. ,convert,5
This command outputs a ready-to-use spaCy model with a Vocab containing the lexical data.,vocab,5
"Construct a Doc object from vocab, words and spaces.","doc = Doc(nlp.vocab, words=words, spaces=spaces)",5
The text in the last element in doc.,doc[-1].text,5
Save second and third token in doc into variable span.,span = doc[1:3],5
Make a semantic similarity estimate. The default estimate is cosine similarity using an average of word vectors.,Doc.similarity(),5
Returns the semantic similarity between Docs apples and oranges.,apples.similarity(oranges),5
"Load attributes from a numpy array. Write to a Doc object, from an (M, N) array of attributes.",Doc.from_array(),5
"Save the current state to directory ""/path/to/doc"".","doc.to_disk(""/path/to/doc"")",5
"Iterate over the sentences in the document. Sentence spans have no label. To improve accuracy on informal texts, spaCy calculates sentence boundaries from the syntactic dependency parse. If the parser is disabled, the sents iterator will be unavailable.",Doc.sents,5
A unicode representation of the document text.,text,5
"Export given token attributes to a numpy ndarray. If attr_ids is a sequence of M attributes, the output array will be of shape (N, M), where N is the length of the Doc (in tokens). If attr_ids is a single attribute, the output shape will be (N,). You can specify attributes by integer ID (e.g. spacy.attrs.LEMMA) or string name (e.g. ‘LEMMA’ or ‘lemma’). The values will be 64-bit integers.

Returns a 2D array with one row per token and one column per attribute (when attr_ids is a list), or as a 1D numpy array, with one item per attribute (when attr_ids is a single value).",Doc.to_array(),5
"The document’s local memory heap, for all C data it owns.",mem,5
Container for dense vector representations.,tensor,5
"A generic storage area, for user custom data.",user_data,5
Language of the document’s vocabulary (int).,lang,5
Language of the document’s vocabulary (unicode).,lang_,5
A flag indicating that the document has been part-of-speech tagged.,is_tagged,5
A flag indicating that the document has been syntactically parsed.,is_parsed,5
A flag indicating that sentence boundaries have been applied to the document.,is_sentenced,5
"A flag indicating that named entities have been set. Will return True if any of the tokens has an entity tag set, even if the others are unknown.",is_nered,5
"The document’s positivity/negativity score, if available.",sentiment,5
A dictionary that allows customization of the Doc’s properties.,user_hooks,5
A dictionary that allows customization of properties of Token children.,user_token_hooks,5
A dictionary that allows customization of properties of Span children.,user_span_hooks,5
User space for adding custom attribute extensions.,_,5
The index of the token within the document.,offset,5
A boolean value indicating whether the token starts a sentence. None if unknown. Defaults to True for the first token in the Doc.,is_sent_start,5
The sentence span that this token is a part of.,sent,5
"The named entities in the document. Returns a tuple of named entity Span objects, if the entity recognizer has been applied.",Doc.ents,5
"An alias of Doc.text, provided for duck-type compatibility with Span and Token.",text_with_ws,5
"Maps either a label to a score for categories applied to whole document, or (start_char, end_char, label) to score for categories applied to spans. start_char and end_char should be character offsets, label can be either a string or an integer ID, and score should be a float.",cats,5
Trailing space character if present.,whitespace_,5
"The syntactic parent, or “governor”, of this token.",head,5
Named entity type (int).,ent_type,5
Named entity type (unicode).,ent_type_,5
"IOB code of named entity tag. 3 means the token begins an entity, 2 means it is outside an entity, 1 means it is inside an entity, and 0 means no entity tag is set (int).",ent_iob,5
"IOB code of named entity tag. “B” means the token begins an entity, “I” means it is inside an entity, “O” means it is outside an entity, and """" means no entity tag is set (unicode).",ent_iob_,5
"Base form of the token, with no inflectional suffixes (int).",lemma,5
"Base form of the token, with no inflectional suffixes (unicode).",lemma_,5
Does the token consist of whitespace characters? Equivalent to token.text.isspace().,is_space,5
Does the token resemble a URL?,like_url,5
"Does the token represent a number? e.g. “10.9”, “10”, “ten”, etc.",like_num,5
Does the token resemble an email address?,like_email,5
Is the token out-of-vocabulary?,is_oov,5
Coarse-grained part-of-speech.,pos_,5
Fine-grained part-of-speech.,tag_,5
Syntactic dependency relation.,dep_,5
Language of the parent document’s vocabulary.,lang_,5
Smoothed log probability estimate of token’s word type (context-independent entry in the vocabulary).,prob,5
The index of the first token after the span.,end,5
A meaning representation of the span.,vector,5
The character offset for the start of the span.,start_char,5
The character offset for the end of the span.,end_char,5
"Process texts as a stream, and yield Doc objects in order. This is usually more efficient than processing texts one-by-one.",Language.pipe,5
A sequence of unicode objects.,texts,5
The index of the first token of the span.,start,5
"If set to True, inputs should be a sequence of (text, context) tuples. Output will then be a sequence of (doc, context) tuples. Defaults to False.",as_tuples,5
The number of texts to buffer.,batch_size,5
"Config parameters for specific pipeline components, keyed by component name.",component_cfg,5
"Number of processors to use, only supported in Python 3. Defaults to 1.",n_process,5
