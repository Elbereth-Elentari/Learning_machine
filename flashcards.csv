Return the value of the first href tag.,"link.get(""href"")",4
The character offset of the token within the parent document.,idx,4
Locate the first element with a specific id.,driver.find_element_by_id(),4
Locate all elements with a specific name.,driver.find_elements_by_name(),4
Locate all elements with a specific xpath.,driver.find_elements_by_xpath(),4
Import By.,from selenium.webdriver.common.by import By,4
"Return the stream of the Language nlp, obtained from texts and with batch_size 50.","nlp.pipe(texts, batch_size=50)",4
Update the models in the pipeline.,Language.update,4
"A batch of Doc objects or unicode. If unicode, a Doc object will be created from the text.",docs,4
"A batch of GoldParse objects or dictionaries. Dictionaries will be used to create GoldParse objects. For the available keys and their usage, see GoldParse.__init__.",golds,4
The dropout rate.,drop,4
An optimizer.,sgd,4
"Dictionary to update with the loss, keyed by pipeline component.",losses,4
Evaluate a model’s pipeline components.,Language.evaluate,4
"Tuples of Doc and GoldParse objects or (text, annotations) of raw text and a dict (see simple training style).",docs_golds,4
Print debugging information.,verbose,4
"Creating a Series by passing a list of values, letting pandas create a default integer index.",pd.Series([]),4
View bottom 3 rows of df.,df.tail(3),4
Calculate the mean of each value in df.,df.mean(),4
"Grouping df by column A and B, and then applying the sum function.","df.groupby([""A"", ""B""]).sum()",4
"Produce a pivot table from df, taking values from column D, indices from columns A and B, and columns from column C.","pd.pivot_table(df, values=""D"", index=[""A"", ""B""], columns=[""C""])",4
Tokenize the string text into words.,word_tokenize(text),4
Import sentence tokenizer.,from nltk.tokenize import sent_tokenize,4
Tokenize the string text into sentences.,sent_tokenize(text),4
foreign word,FW,4
"adjective, comparative",JJR,4
"Optional Scorer to use. If not passed in, a new one will be created.",scorer,4
"Allocate models, pre-process training data and acquire an optimizer.",Language.begin_training,4
Gold-standard training data.,gold_tuples,4
"Replace weights of models in the pipeline with those provided in the params dictionary. Can be used as a context manager, in which case, models go back to their original weights after the block.",Language.use_params,4
A dictionary of parameters keyed by model ID.,params,4
"Can be called before training to pre-process gold data. By default, it handles nonprojectivity and adds missing tags to the tag map.",Language.preprocess_gold,4
Callable that takes a unicode text and returns a Doc.,make_doc,4
infinite marker,TO,4
verb gerund,VBG,4
An entry-point for WSGI-compatible web servers to serve your project.,mysite/wsgi.py,4
A framework for managing static files.,django.contrib.staticfiles,4
"Return all non-overlapping matches of pattern in string, as a list of strings.","re.findall(pattern, string)",4
Deserialize fp (a .read()-supporting text file or binary file containing a JSON document) to a Python object,json.load(fp),4
"Test that obj is not an instance of cls (which can be a class or a tuple of classes, as supported by isinstance()). To check for the exact type, use assertIs(type(obj), cls).","assertNotIsInstance(obj, cls)",4
"List of (name, component) tuples describing the current processing pipeline, in order.",pipeline,4
"List of pipeline component names, in order.",pipe_names,4
"List of labels set by the pipeline components, if available, keyed by component name.",pipe_labels,4
"Custom meta data for the Language class. If a model is loaded, contains meta data of the model.",meta,4
"Factories that create pre-defined pipeline components, e.g. the tagger, parser or entity recognizer, keyed by their component name.",factories,4
Tokenization rules and exceptions.,tokenizer,4
Exceptions and special-cases for the tokenizer.,rules,4
A function matching the signature of re.compile(string).search to match prefixes.,prefix_search,3
A function matching the signature of re.compile(string).search to match suffixes.,suffix_search,4
A function matching the signature of re.compile(string).finditer to find infixes.,infix_finditer,4
Create an instance of Firefox WebDriver.,driver = webdriver.Firefox(),4
"Select the option with the value ""value"".",select.select_by_value(value),4
"Move to frame ""frameName"".","driver.switch_to_frame(""frameName"")",4
Go back to the main frame.,driver.switch_to_default_content(),4
Move forward in the browser history.,driver.forward(),4
Set a cookie named cookie.,driver.add_cookie(cookie),4
Locate the first element with a specific link text.,driver.find_element_by_link_text(),4
Locate the first element with a specific tag name.,driver.find_element_by_tag_name(),4
Locate the first element with a specific class name.,driver.find_element_by_class_name(),4
Import WebDriverWait.,from selenium.webdriver.support.ui import WebDriverWait,4
A function matching the signature of `re.compile(string).match to find token matches.,token_match,4
Create a blank Tokenizer with just the English vocab,tokenizer = Tokenizer(nlp.vocab),4
import Tokenizer,from spacy.tokenizer import Tokenizer,4
"Create a Tokenizer with the default settings for English, including punctuation rules and exceptions.",tokenizer = nlp.Defaults.create_tokenizer(nlp),4
Tokenize a stream of texts.,Tokenizer.pipe,3
Find internal split points of the string.,Tokenizer.find_infix,3
"Find the length of a prefix that should be segmented from the string, or None if no prefix rules match.",Tokenizer.find_prefix,3
"Find the length of a suffix that should be segmented from the string, or None if no suffix rules match.",Tokenizer.find_suffix,3
Waiting condition that the title has a certain value.,title_is,4
View top 5 rows of df.,df.head(),4
Display the index of df.,df.index,4
"Sort df by axis 1, in descending order.","df.sort_index(axis=1, ascending=False)",4
"Select rows with value greater than 0 in column ""A"", in df.","df[df[""A""] > 0]",4
Grouping df by column A and then applying the sum function to the resulting groups.,"df.groupby(""A"").sum()",3
coordinating conjunction,CC,3
existential there,EX,3
"The lookups object containing the (optional) tables ""lemma_rules"", ""lemma_index"", ""lemma_exc"" and ""lemma_lookup"".",lookups,3
Import Lemmatizer.,from spacy.lemmatizer import Lemmatizer,3
Import Lookups.,from spacy.lookups import Lookups,3
"Add a lemmatization rule for nouns deleting final ""s"".","lookups.add_table(""lemma_rules"", {""noun"": [[""s"", """"]]})",3
The token’s universal part-of-speech tag.,univ_pos,3
"Look up a lemma in the lookup table, if available. If no lemma is found, the original string is returned. Languages can provide a lookup table via the Lookups.",Lemmatizer.lookup,3
"Check whether we’re dealing with an uninflected paradigm, so we can avoid lemmatization entirely.",is_base_form,3
"proper noun, plural",NNPS,3
personal pronoun,PRP,3
verb,VB,3
wh- adverb (how),WRB,3
Return the stem of word w.,PorterStemmer().stem(w),3
Import WordNet.,from nltk.corpus import wordnet,3
Draw a plot of frequency distribution fd.,fd.plot(),3
Initialize a model for the pipe. The model should implement the thinc.neural.Model API. Wrappers are under development for most major machine learning libraries.,Tagger.Model,3
"The model powering the pipeline component. If no model is supplied, the model is created when you call begin_training, from_disk or from_bytes.",model,3
Construction of the tagger model via create_pipe.,"tagger = nlp.create_pipe(""tagger"")",3
Import Tagger.,from spacy.pipeline import Tagger,3
Initialize the tagger model from disk.,"tagger.from_disk(""/path/to/model"")",3
Apply the pipe to a stream of documents. This usually happens under the hood when the nlp object is called on a text and all pipeline components are applied to the Doc in order. Both __call__ and pipe delegate to the predict and set_annotations methods.,Tagger.pipe,3
A stream of documents.,stream,3
"Apply the pipeline’s model to a batch of docs, without modifying them.",Tagger.predict,3
Import admin.,from django.contrib import admin,3
Returns a list of all selected options.,select.all_selected_options,3
"This waits up to 10 seconds before throwing a TimeoutException unless it finds the element by id ""myDynamicElement"", to return within 10 seconds.","element = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, ""myDynamicElement"")))",3
Select row number 3 in df.,df.iloc[3],3
Read file foo.csv.,"pd.read_csv(""foo.csv"")",3
cardinal digit,CD,3
"noun, singular",NN,3
adverb,RB,3
"adverb, comparative",RBR,3
interjection (goodbye),UH,3
"Modify a batch of documents, using pre-computed scores.",Tagger.set_annotations,3
Find the loss and gradient of loss for the batch of documents and their predicted scores.,Tagger.get_loss,3
Add a new label to the pipe.,Tagger.add_label,3
"Add a new label ""MY_LABEL"" for nouns.","tagger.add_label(""MY_LABEL"", {POS: 'NOUN'})",3
"The labels currently added to the component. Note that even for a blank component, this will always include the built-in coarse-grained part-of-speech tags by default, e.g. VERB, NOUN and so on.",Tagger.labels,3
Define the knowledge base (KB) used for disambiguating named entities to KB identifiers.,Entity_Linker.set_kb,3
Make categories mutually exclusive. Defaults to False.,exclusive_classes,3
"Text classification models can be used to solve a wide variety of problems. Differences in text length, number of labels, difficulty, and runtime performance constraints mean that no single algorithm performs well on all types of problems. To handle a wider variety of problems, the TextCategorizer object allows configuration of its model architecture, using the architecture keyword argument.",Architectures,3
Default: Stacked ensemble of a bag-of-words model and a neural network model. The neural network uses a CNN with mean pooling and attention. The “ngram_size” and “attr” arguments can be used to configure the feature extraction for the bag-of-words model.,ensemble,3
"A neural network model where token vectors are calculated using a CNN. The vectors are mean pooled and used as features in a feed-forward network. This architecture is usually less accurate than the ensemble, but runs faster.",simple_cnn,2
"wh-determiner (that, what)",WDT,2
Import the Regex Parser.,from nltk import RegexpParser,2
Import views.,from . import views,2
An empty file that tells Python that this directory should be considered a Python package.,mysite/__init__.py,2
The admin site.,django.contrib.admin,2
Import models.,from django.db import models,2
Import Render.,from django.shortcuts import render,2
always return this after successfully dealing with POST data,HttpResponseRedirect,2
A family of open system standards based on Unix.,POSIX,2
"A command that is implemented internally by the shell itself, rather than by an executable program somewhere in the file system.",builtin,2
"An ngram “bag-of-words” model. This architecture should run much faster than the others, but may not be as accurate, especially if texts are short. The features extracted can be controlled using the keyword arguments ngram_size and attr. For instance, ngram_size=3 and attr=""lower"" would give lower-cased unigram, trigram and bigram features. 2, 3 or 4 are usually good choices of ngram size.",bow,2
Validate all patterns added to this matcher.,validate,2
"Yield the match lists along with the docs, making results (doc, matches) tuples.",return_matches,2
"Interpret the input stream as (doc, context) tuples, and yield (result, context) tuples out. If both return_matches and as_tuples are True, the output will be a sequence of ((doc, matches), context) tuples.",as_tuples,2
"Get the number of rules added to the matcher. Note that this only returns the number of rules (identical with the number of IDs), not the number of individual patterns.",Matcher.__len__,2
Check whether the matcher contains rules for a match ID.,Matcher.__contains__,2
A mechanism by which a process may be notified by the kernel of an event occurring in the system.,signal,2
double quote,"\""",2
a control-x character,\cx,2
"The test-commands list is executed, and if its return status is zero, the consequent-commands list is executed. If test-commands returns a non-zero status, each elif list is executed in turn, and if its exit status is zero, the corresponding more-consequents is executed and the command completes. If ‘else alternate-consequents’ is present, and the final command in the final if or elif clause has a non-zero exit status, then alternate-consequents is executed. The return status is the exit status of the last command executed, or zero if no condition tested true.",if test-commands; then consequent-commands; [elif more-test-commands; then more-consequents;] [else alternate-consequents;] fi,2
"A Flatten layer, with a square input shape with 28 values per side.","tf.keras.layers.Flatten(input_shape=(28, 28))",2
Identify the 'survived' column as the one with the predicted value.,LABEL_COLUMN = 'survived',2
"Merge dataframes left and right on key ""key"".","pd.merge(left, right, on=""key"")",2
"Add a rule to the matcher, consisting of an ID key, one or more patterns, and a callback function to act on the matches. The callback function will receive the arguments matcher, doc, i and matches. If a pattern already exists for the given ID, the patterns will be extended. An on_match callback will be overwritten.",Matcher.add,2
An ID for the thing you’re matching.a,match_id,2
"Callback function to act on matches. Takes the arguments matcher, doc, i and matches.",on_match,2
"Match pattern. A pattern consists of a list of dicts, where each dict describes a token.",*patterns,2
Remove a rule from the matcher. A KeyError is raised if the match ID does not exist.,Matcher.remove,2
"Retrieve the pattern stored for a key. Returns the rule as an (on_match, patterns) tuple containing the callback and available patterns.",Matcher.get,2
Optional attr to pass to the internal PhraseMatcher. defaults to None,phrase_matcher_attr,2
"If existing entities are present, e.g. entities added by the model, overwrite them by matches if necessary. Defaults to False.",overwrite_ents,2
"Execute consequent-commands as long as test-commands has an exit status of zero. The return status is the exit status of the last command executed in consequent-commands, or zero if none was executed.",while test-commands; do consequent-commands; done,2
"Expand words (see Shell Expansions), and execute commands once for each member in the resultant list, with name bound to the current member. If ‘in words’ is not present, the for command executes the commands once for each positional parameter that is set, as if ‘in ""$@""’ had been specified (see Special Parameters).",for name [ [in [words ...] ] ; ] do commands; done,2
"Matches the empty string, but only at the beginning or end of a word.",\b,2
Test whether every element in other is in the set.,set >= other,2
"An idealized naive date, assuming the current Gregorian calendar always was, and always will be, in effect. Attributes: year, month, and day.",datetime.date,2
"This static method returns a translation table usable for str.translate().

If there is only one argument, it must be a dictionary mapping Unicode ordinals (integers) or characters (strings of length 1) to Unicode ordinals, strings (of arbitrary lengths) or None. Character keys will then be converted to ordinals.

If there are two arguments, they must be strings of equal length, and in the resulting dictionary, each character in x will be mapped to the character at the same position in y. If there is a third argument, it must be a string, whose characters will be mapped to None in the result.","str.maketrans(x, y, z)",2
Remove all elements from the set.,clear(),2
Causes the resulting RE to match 0 or 1 repetitions of the preceding RE.,?,2
drops you into the debugger at the call site,breakpoint(),2
The underlying matcher used to process token patterns.,matcher,1
"The underlying phrase matcher, used to process phrase patterns.",phrase_matcher,1
"Causes the resulting RE to match from m to n repetitions of the preceding RE, attempting to match as many repetitions as possible.","{m, n}",1
Return a str version of object.,str(),1
Return the “identity” of an object. This is an integer which is guaranteed to be unique and constant for this object during its lifetime.,id(),1
"The token patterns present in the entity ruler, keyed by label.",token_patterns,1
"The phrase patterns present in the entity ruler, keyed by label.",phrase_patterns,1
"Optional custom list of punctuation characters that mark sentence ends. Defaults to [""."", ""!"", ""?""].",punct_chars,1
"Merge noun chunks into a single token. Also available via the string name ""merge_noun_chunks"". After initialization, the component is typically added to the processing pipeline using nlp.add_pipe.",merge_noun_chunks,1
"Read foo.xlsx, Sheet1, with no column index, and empty cells filled with ""NA"".","pd.read_excel(""foo.xlsx"", ""Sheet1"", index_col=None, na_values=[""NA""])",1
Import word tokenizer.,from nltk.tokenize import word_tokenize,1
preposition/subordinating conjunction,IN,1
possessive ending,POS,1
